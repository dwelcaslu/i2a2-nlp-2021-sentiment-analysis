{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Loading useful libraries and modules","metadata":{}},{"cell_type":"code","source":"import re\nimport string\n\nimport pandas as pd\nimport numpy as np\n\nimport nltk\nfrom nltk.tokenize import TweetTokenizer\n\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import LinearSVC\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier\n\nfrom xgboost import XGBClassifier\nfrom lightgbm import LGBMClassifier\n\nfrom pprint import pprint\nfrom tqdm.notebook import tqdm","metadata":{"trusted":true},"execution_count":1,"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<style type='text/css'>\n.datatable table.frame { margin-bottom: 0; }\n.datatable table.frame thead { border-bottom: none; }\n.datatable table.frame tr.coltypes td {  color: #FFFFFF;  line-height: 6px;  padding: 0 0.5em;}\n.datatable .bool    { background: #DDDD99; }\n.datatable .object  { background: #565656; }\n.datatable .int     { background: #5D9E5D; }\n.datatable .float   { background: #4040CC; }\n.datatable .str     { background: #CC4040; }\n.datatable .row_index {  background: var(--jp-border-color3);  border-right: 1px solid var(--jp-border-color0);  color: var(--jp-ui-font-color3);  font-size: 9px;}\n.datatable .frame tr.coltypes .row_index {  background: var(--jp-border-color0);}\n.datatable th:nth-child(2) { padding-left: 12px; }\n.datatable .hellipsis {  color: var(--jp-cell-editor-border-color);}\n.datatable .vellipsis {  background: var(--jp-layout-color0);  color: var(--jp-cell-editor-border-color);}\n.datatable .na {  color: var(--jp-cell-editor-border-color);  font-size: 80%;}\n.datatable .footer { font-size: 9px; }\n.datatable .frame_dimensions {  background: var(--jp-border-color3);  border-top: 1px solid var(--jp-border-color0);  color: var(--jp-ui-font-color3);  display: inline-block;  opacity: 0.6;  padding: 1px 10px 1px 5px;}\n</style>\n"},"metadata":{}}]},{"cell_type":"markdown","source":"## Loading training and testing data","metadata":{"id":"NqGKSeshFYD9"}},{"cell_type":"code","source":"# Load train data\nusecols = ['id', 'review_title', 'review_text']\nraw_train_data = pd.read_csv('../input/i2a2-nlp-2021-sentiment-analysis/train.csv', index_col='id', usecols=usecols + ['rating'])\n\n# Load test data\nraw_test_data = pd.read_csv('../input/i2a2-nlp-2021-sentiment-analysis/train.csv', index_col='id', usecols=usecols)","metadata":{"id":"kTKyYSh5w63Q","trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"# Concatenate text features\nraw_train_data['review'] = raw_train_data['review_title'] + ' ' + raw_train_data['review_text']\nraw_test_data['review'] = raw_test_data['review_title'] + ' ' + raw_test_data['review_text']\n\nwith pd.option_context('display.max_colwidth', None):\n    display(raw_train_data[['review', 'rating']].head())","metadata":{"id":"zYPWM1YMSptn","outputId":"880b9c8a-1147-4848-b35d-4875a2c11c24","trusted":true},"execution_count":3,"outputs":[{"output_type":"display_data","data":{"text/plain":"                                                                                                                                                                                                                                                                                                                                                                          review  \\\nid                                                                                                                                                                                                                                                                                                                                                                                 \n0                                                                                                             Americanas Nao tem compromisso Comprei um TV Box dia 14 /12/ 2017 e ate agora as americanas não Devovel o meu dinheiro. Ja Fiz de tudo pre entrar em contato com essa empresa mas os fones que  sao disponiveis por ela nao existem. pre mim isso se chama CALOTE.   \n1                                                                                                                                                                                                                                                                                                     muito bom produto Considerando seu custo, é ótimo som com muita qualidade.   \n2   Excelente! Simplesmente é a melhor que já vi... ela é linda, grande, fácil de montar e praticamente ela se autoconfigura sozinha. Parece que vc está no computador. O som é ótimo e a definição de imagem me deixou apaixonada. A entrega foi muito rápida e gostei muito da presteza e agilidade da transportadora Plimor. Pude acompanhar cada passo da entrega. Parabéns!   \n3                                                                                                                                                                                                                                                                                             Entrega antes do prazo! Estou muito satisfeita com o produto é a entrega no prazo.   \n4                                                                                                                                                                                                                                                           Muito bonito. A gaiola é muito bonita. Colorida e grande. Algumas grades ficaram meio soltas,mas ainda valeu a pena.   \n\n    rating  \nid          \n0        1  \n1        4  \n2        5  \n3        4  \n4        3  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>review</th>\n      <th>rating</th>\n    </tr>\n    <tr>\n      <th>id</th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Americanas Nao tem compromisso Comprei um TV Box dia 14 /12/ 2017 e ate agora as americanas não Devovel o meu dinheiro. Ja Fiz de tudo pre entrar em contato com essa empresa mas os fones que  sao disponiveis por ela nao existem. pre mim isso se chama CALOTE.</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>muito bom produto Considerando seu custo, é ótimo som com muita qualidade.</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Excelente! Simplesmente é a melhor que já vi... ela é linda, grande, fácil de montar e praticamente ela se autoconfigura sozinha. Parece que vc está no computador. O som é ótimo e a definição de imagem me deixou apaixonada. A entrega foi muito rápida e gostei muito da presteza e agilidade da transportadora Plimor. Pude acompanhar cada passo da entrega. Parabéns!</td>\n      <td>5</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Entrega antes do prazo! Estou muito satisfeita com o produto é a entrega no prazo.</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Muito bonito. A gaiola é muito bonita. Colorida e grande. Algumas grades ficaram meio soltas,mas ainda valeu a pena.</td>\n      <td>3</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"## Exploratory data analysis","metadata":{}},{"cell_type":"code","source":"corpus = ' '.join([*raw_train_data['review']])\nvocab = nltk.FreqDist(TweetTokenizer().tokenize(corpus))","metadata":{"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"print(f'The training data has {len(vocab):,} unique words.', '---' , sep='\\n', end='\\n\\n')\n\n# Show the 100 most common words\npprint(vocab.most_common(100),  compact=True)","metadata":{"trusted":true},"execution_count":26,"outputs":[{"name":"stdout","text":"The training data has 69,407 unique words.\n---\n\n[('.', 139325), (',', 134038), ('e', 75133), ('o', 68277), ('de', 65344),\n ('!', 58789), ('a', 56868), ('produto', 56634), ('que', 46407), ('não', 40569),\n ('muito', 38656), ('do', 35955), ('é', 32916), ('com', 30187), ('bom', 24561),\n ('para', 23500), ('um', 22703), ('Produto', 17562), ('da', 17371),\n ('O', 16326), ('em', 16077), ('no', 15774), ('entrega', 14969), ('na', 13602),\n ('qualidade', 13323), ('mais', 12977), ('...', 12698), ('uma', 12671),\n ('mas', 12405), ('Não', 12036), ('bem', 12021), ('recomendo', 12004),\n ('prazo', 11466), ('Muito', 10800), ('Gostei', 10595), ('as', 10429),\n ('foi', 10361), ('A', 10037), ('chegou', 9429), ('excelente', 9063),\n ('antes', 8826), ('eu', 8534), ('se', 8329), ('tem', 8327), ('recebi', 8170),\n ('Excelente', 7937), ('Ótimo', 7638), ('por', 7581), ('meu', 7562),\n ('como', 7431), ('Recomendo', 7249), ('me', 6988), ('compra', 6808),\n ('minha', 6719), ('veio', 6657), ('boa', 6556), ('ótimo', 6550), ('pra', 6472),\n ('os', 6286), ('dia', 6261), ('sem', 6221), ('gostei', 6033), ('até', 5938),\n ('já', 5812), ('ainda', 5733), ('super', 5731), ('ser', 5628), ('E', 5400),\n ('pois', 5371), ('mesmo', 5125), ('?', 5110), ('rápida', 5072), ('Bom', 5067),\n ('comprei', 4887), ('dias', 4682), ('ao', 4590), ('comprar', 4541),\n ('preço', 4536), ('só', 4524), ('expectativas', 4497), ('ele', 4445),\n ('minhas', 4437), ('Americanas', 4428), ('aparelho', 4396), ('estou', 4390),\n ('está', 4360), ('nao', 4329), ('esse', 4245), ('custo', 4192), (')', 4134),\n ('rápido', 4114), ('Comprei', 4107), ('(', 4098), ('ótima', 4046),\n ('pelo', 4005), ('nada', 3990), ('loja', 3856), ('NÃO', 3764), ('site', 3740),\n ('nem', 3734)]\n","output_type":"stream"}]},{"cell_type":"code","source":"# Show the 100 least common words\npprint(vocab.most_common()[:-101:-1],  compact=True)","metadata":{"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"[('MONTAGEM.ME', 1), ('parebens', 1), ('reclamar.Pra', 1), ('retificar', 1),\n ('totalizar', 1), ('contabilizando', 1), ('encurvados', 1), ('Vergonhosa', 1),\n ('Surprendente', 1), ('locar', 1), ('morava', 1), ('finda', 1),\n ('Desamassa', 1), ('AJUDE', 1), ('escovo', 1), ('pesquisávamos', 1),\n ('HX318C10FR', 1), ('CL10', 1), ('1866Mhz', 1), ('FURY', 1), ('MU6120', 1),\n ('ort', 1), ('8-p', 1), ('todo.um', 1), ('17,18', 1), ('traições', 1),\n ('dramas', 1), ('korai', 1), ('Mastercad', 1), ('credores', 1), ('sugunda', 1),\n ('CERCA', 1), ('performa', 1), ('excelente.Consistente', 1), ('ahshsh', 1),\n ('Caçadores', 1), ('Veda', 1), ('descepicionado', 1), ('recebi.Estou', 1),\n ('EDN', 1), ('FECHA', 1), ('327', 1), ('ELM', 1), ('TELEFONEMAS', 1),\n ('Inconformada', 1), ('cente', 1), ('fixá-la', 1), ('sgestão', 1),\n ('Inutilizando', 1), ('agor', 1), ('inrresponssabilidade', 1),\n ('Descepção', 1), ('flashes', 1), ('Mamiss', 1), ('cumpádis', 1),\n ('casebre', 1), ('lampiões', 1), ('pavios', 1), ('geradorzinho', 1),\n ('mega-promoção', 1), ('gritava', 1), ('manoellgs@yahoo.com.br', 1),\n ('5200', 1), ('99144', 1), ('MANOEL', 1), ('AJUDASSEM', 1), ('SOUBERAM', 1),\n ('APAGADA', 1), ('LIDA', 1), ('RÓTULO', 1), ('fOi', 1), ('roupiroupinhas', 1),\n ('Edtou', 1), ('Silvestre', 1), ('Lucio', 1), ('consertei', 1), ('Aparlho', 1),\n ('ofereces', 1), ('Ficarei', 1), ('Cromada', 1), ('proprios', 1),\n ('coadores', 1), ('4.99', 1), ('ncomprei', 1), ('cedendo', 1), ('saltadas', 1),\n ('capturadas', 1), ('empenamento', 1), ('durinha', 1), ('companheiros', 1),\n ('vvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvv',\n  1),\n ('axhei', 1), ('Detran', 1), ('reponde', 1), ('duais', 1), ('pasaram', 1),\n ('avançadinha', 1), ('DSRL', 1), ('explicaram', 1), ('Seg', 1)]\n","output_type":"stream"}]},{"cell_type":"code","source":"long_words = [word for word in vocab if len(word) > 20]\n\n# Show some long words\npprint(long_words[:10], compact=True)","metadata":{"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"['https://www.americanas.com.br/',\n 'xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx', 'LARANJAS.ASSIST.TECNICA',\n 'Uhuhuhuhuhuhuhihuuhuuhuhuhuuhuhuhuh',\n 'hjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjjj',\n 'xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx',\n 'tooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooop',\n 'xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx',\n 'xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx',\n 'HUAHAUHUHAUAHUAHUAHUAHUAHAUHAUAHUAHUAHUAHUAHAUHAUHAUHAUHAU']\n","output_type":"stream"}]},{"cell_type":"code","source":"short_words = [word for word in vocab if len(word) <= 3]\n\n# Show some short words\npprint(short_words[:110], compact=True)","metadata":{"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"['Nao', 'tem', 'um', 'TV', 'Box', 'dia', '14', '/', '12', 'e', 'ate', 'as',\n 'não', 'o', 'meu', '.', 'Ja', 'Fiz', 'de', 'pre', 'em', 'com', 'mas', 'os',\n 'que', 'sao', 'por', 'ela', 'nao', 'mim', 'se', 'bom', 'seu', ',', 'é', 'som',\n '!', 'a', 'já', 'vi', '...', 'vc', 'no', 'O', 'me', 'A', 'foi', 'da', 'do',\n 'hp', 'sem', '..', 'Boa', 'ele', 'nem', 'bem', 'boa', 'Com', 'eu', 'pra', 'q',\n 'só', 'voz', '10', 'fiz', 'As', 'na', 'Vc', 'qdo', 'vê', 'Ela', 'Mas', 'tá',\n 'Tô', 'ano', 'uso', 'J5', 'Não', 'ao', 'É', 'mão', 'era', 'eté', '22', '06',\n 'Já', 'ser', '(', ')', 'Eu', 'sou', 'até', 'hj', 'n', 'deu', 'E', 'uma', 'há',\n 'ou', 'vou', 'B', 'aos', 'sai', 'dor', 'fiu', 'BOM', 'JÁ', 'UM', 'Só', 'ter']\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Preprocessing raw text data","metadata":{}},{"cell_type":"code","source":"# Compile regular expressions\nremove_url = re.compile(r'https?://\\S+|www\\.\\S+')\nremove_email = re.compile(r'\\S+@\\S+')\nremove_duplicate_word = re.compile(r'\\b(\\w+?)\\1+')\nremove_duplicate_char = re.compile(r'([^rs])(?=\\1+)|(r)(?=r{2,})|(s)(?=s{2,})')\nremove_number = re.compile(r'\\d+')\nremove_extra_space = re.compile(r'\\s+')\n\n# Load punctuation\npunctuation = [*string.punctuation]\npunctuation.extend(['º', 'ª'])\n\ndef preprocess_text(text: str) -> str:\n\n    # Convert to lowercase\n    text = text.lower() \n    \n    # Apply regular expressions\n    text = remove_url.sub('', text)    # remove urls\n    text = remove_email.sub('', text)  # remove emails\n    text = remove_duplicate_word.sub(r'\\1', text) # remove duplicate words\n    text = remove_duplicate_char.sub('', text)    # remove duplicate chars; except \"rr\" and \"ss\" digraphs\n    text = remove_number.sub(' ', text)  # remove numbers\n\n    ## Expand abbreviatons\n    text = re.sub(r'\\b(n|ñ)([aãâ]o)?\\b', ' não ', text)\n    text = re.sub(r'\\bt[aá]\\b', ' está ', text)\n    text = re.sub(r'\\b(p(r[oaá])?)\\b', ' para ', text)\n    text = re.sub(r'\\bq\\b', ' que ', text)\n    text = re.sub(r'\\bpq[s]?\\b', ' porque ', text)\n    text = re.sub(r'\\btb[m|n]?\\b', ' também ', text)\n    text = re.sub(r'\\vc[s]?\\b', ' você ', text)\n    text = re.sub(r'\\bmt[ao]?s?\\b', ' muito ', text)\n    text = re.sub(r'\\b(p(r[oaá]s?)?)\\b', ' para ', text)\n    text = re.sub(r'\\bhj\\b', ' hoje ', text)\n    text = re.sub(r'\\bobs\\b', ' observação ', text)\n    text = re.sub(r'\\beh\\b', ' é ', text)\n\n    # Preprocess long words\n    text = re.sub(r'(ótimo)[v]?\\1+', r' \\1 ', text)\n    text = re.sub(r'\\b(ok)\\1+', r' \\1 ', text)\n    text = re.sub(r'\\b(natural)', r' \\1 ', text)\n    text = re.sub(r'(((bla)(ba)?)+\\2)', r' \\2 ', text)\n    text = re.sub(r'(altura|largura)x', r' \\1 ', text)\n    text = re.sub(r'(compra|embalagem)x', r' \\1 ', text) \n    text = re.sub(r'(ruim|regular|bom|[oó]timo|excelente)', r' \\1 ', text)\n\n    # Remove trailing spaces\n    text = remove_extra_space.sub(' ', text)\n\n    # Instatiate a TweetTokenizer object\n    tokenizer = TweetTokenizer(preserve_case=False, # lowercasing\n                               reduce_len=True, \n                               strip_handles=True)  # remove mentions\n                               \n    # Tokenize the text    \n    tokens = tokenizer.tokenize(text)\n\n    # Remove punctuation\n    tokens = [token for token in tokens if token not in punctuation]\n    \n    # Remove non-alphabetic and longer than twenty chars words\n    tokens = [token for token in tokens if token.isalpha() and len(token) <= 20]\n\n    return ' '.join(tokens)","metadata":{"id":"ckOUHp-G74uU","trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"%%time\nraw_train_data['review_clean'] = raw_train_data['review'].apply(preprocess_text)\nraw_test_data['review_clean'] = raw_test_data['review'].apply(preprocess_text)","metadata":{"trusted":true},"execution_count":10,"outputs":[{"name":"stdout","text":"CPU times: user 1min 57s, sys: 11.3 ms, total: 1min 57s\nWall time: 1min 57s\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## After data preprocessing","metadata":{}},{"cell_type":"code","source":"corpus = ' '.join([*raw_train_data['review_clean']])\nvocab = nltk.FreqDist(TweetTokenizer().tokenize(corpus))","metadata":{"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"print(f'The training data has {len(vocab):,} unique words.', '---' , sep='\\n', end='\\n\\n')\n\n# Show the 100 most common words\npprint(vocab.most_common(100),  compact=True)","metadata":{"trusted":true},"execution_count":22,"outputs":[{"name":"stdout","text":"The training data has 42,530 unique words.\n---\n\n[('o', 84646), ('e', 80601), ('produto', 77809), ('de', 68492), ('a', 67199),\n ('não', 63056), ('muito', 52182), ('que', 51063), ('do', 37285), ('é', 36762),\n ('para', 33746), ('com', 31999), ('bom', 31151), ('um', 25072),\n ('recomendo', 20083), ('entrega', 19041), ('da', 18168), ('excelente', 18066),\n ('no', 17381), ('em', 17267), ('gostei', 17102), ('ótimo', 14892),\n ('na', 14806), ('mas', 14620), ('qualidade', 14587), ('uma', 14137),\n ('mais', 13991), ('bem', 12748), ('chegou', 12670), ('prazo', 12031),\n ('as', 11823), ('foi', 11501), ('eu', 11420), ('recebi', 10218), ('se', 9731),\n ('comprei', 9434), ('tem', 9313), ('antes', 9174), ('como', 9034),\n ('meu', 8975), ('por', 8676), ('americanas', 8496), ('super', 8452),\n ('me', 8109), ('boa', 8109), ('já', 8051), ('minha', 7979), ('veio', 7751),\n ('ainda', 7650), ('estou', 7452), ('compra', 7448), ('sem', 7232),\n ('os', 7143), ('até', 7133), ('dia', 6594), ('só', 6300), ('pois', 5853),\n ('ser', 5778), ('ótima', 5641), ('está', 5621), ('mesmo', 5544),\n ('aparelho', 5339), ('rápida', 5323), ('ele', 5302), ('preço', 5087),\n ('esse', 5056), ('dias', 4978), ('ao', 4958), ('custo', 4833),\n ('comprar', 4781), ('celular', 4694), ('expectativas', 4610), ('minhas', 4593),\n ('pelo', 4578), ('tudo', 4520), ('nada', 4477), ('rápido', 4355),\n ('loja', 4339), ('melhor', 4319), ('nem', 4222), ('site', 4015),\n ('fácil', 4014), ('agora', 3955), ('entregue', 3934), ('ou', 3875),\n ('uso', 3845), ('ter', 3827), ('porém', 3721), ('perfeito', 3662),\n ('pouco', 3612), ('isso', 3606), ('amei', 3497), ('adorei', 3484),\n ('quando', 3433), ('todos', 3368), ('bonito', 3307), ('tv', 3304),\n ('são', 3294), ('defeito', 3281), ('das', 3265)]\n","output_type":"stream"}]},{"cell_type":"code","source":"# Show the 100 least common words\npprint(vocab.most_common()[:-101:-1],  compact=True)","metadata":{"trusted":true},"execution_count":13,"outputs":[{"name":"stdout","text":"[('parebens', 1), ('retificar', 1), ('totalizar', 1), ('contabilizando', 1),\n ('encurvados', 1), ('locar', 1), ('morava', 1), ('finda', 1), ('escovo', 1),\n ('pesquisávamos', 1), ('port', 1), ('traições', 1), ('dramas', 1),\n ('mastercad', 1), ('credores', 1), ('sugunda', 1), ('performa', 1),\n ('ahshsh', 1), ('caçadores', 1), ('descepicionado', 1), ('edn', 1), ('elm', 1),\n ('cente', 1), ('sgestão', 1), ('agor', 1), ('inrresponssabilidade', 1),\n ('flashes', 1), ('mamiss', 1), ('cumpádis', 1), ('casebre', 1),\n ('lampiões', 1), ('pavios', 1), ('geradorzinho', 1), ('gritava', 1),\n ('edtou', 1), ('silvestre', 1), ('lucio', 1), ('aparlho', 1), ('ofereces', 1),\n ('proprios', 1), ('coadores', 1), ('ncomprei', 1), ('saltadas', 1),\n ('capturadas', 1), ('empenamento', 1), ('durinha', 1), ('companheiros', 1),\n ('axhei', 1), ('reponde', 1), ('duais', 1), ('pasaram', 1), ('avançadinha', 1),\n ('dsrl', 1), ('explicaram', 1), ('fatando', 1), ('obirgado', 1), ('kiosk', 1),\n ('retornavão', 1), ('pwlo', 1), ('fervimento', 1), ('itensificação', 1),\n ('permaneça', 1), ('seguenciais', 1), ('informarao', 1), ('filias', 1),\n ('risquei', 1), ('frontiline', 1), ('lidinho', 1), ('fucinalmento', 1),\n ('skiny', 1), ('devover', 1), ('cestinhas', 1), ('felizardo', 1), ('csa', 1),\n ('aparalho', 1), ('astolfo', 1), ('triplicou', 1), ('porsivel', 1),\n ('acrtar', 1), ('desobstruir', 1), ('produtoque', 1), ('bugou', 1),\n ('lerdinhos', 1), ('assoaciada', 1), ('nebhum', 1), ('anuciam', 1),\n ('emcontrei', 1), ('orario', 1), ('desaceleração', 1), ('disponibilização', 1),\n ('aparando', 1), ('fervem', 1), ('manifesto', 1), ('encarrega', 1),\n ('porqe', 1), ('bailarina', 1), ('persima', 1), ('elevaram', 1),\n ('comendadissimo', 1), ('ocelular', 1)]\n","output_type":"stream"}]},{"cell_type":"code","source":"short_words = [word for word in vocab if len(word) <= 3]\n\n# Show some short words\npprint(short_words[:105], compact=True)","metadata":{"trusted":true},"execution_count":14,"outputs":[{"name":"stdout","text":"['não', 'tem', 'um', 'tv', 'box', 'dia', 'e', 'ate', 'as', 'o', 'meu', 'ja',\n 'fiz', 'de', 'pre', 'em', 'com', 'mas', 'os', 'que', 'sao', 'por', 'ela',\n 'mim', 'se', 'bom', 'seu', 'é', 'som', 'a', 'já', 'vi', 'vc', 'no', 'me',\n 'foi', 'da', 'do', 'hp', 'sem', 'boa', 'ele', 'nem', 'bem', 'eu', 'só', 'voz',\n 'na', 'qdo', 'vê', 'tô', 'ano', 'uso', 'j', 'ao', 'mão', 'era', 'eté', 'ser',\n 'sou', 'até', 'deu', 'uma', 'há', 'ou', 'vou', 'b', 'aos', 'sai', 'dor', 'fiu',\n 'ter', 'nen', 'são', 'ir', 'ex', 'vem', 'mal', 'sei', 'for', 'vcs', 'tim',\n 'usa', 'g', 'abc', 'x', 'dvd', 'si', 'ok', 'pós', 'az', 'faz', 'vim', 's',\n 'lg', 'sim', 'irá', 'vir', 'ves', 'nas', 'dá', 'tão', 'z', 'hd', 'vez']\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Training and evaluating some models","metadata":{}},{"cell_type":"code","source":"%%time\n\nX = raw_train_data['review_clean']\ny = raw_train_data['rating']\n\nvectorizer = TfidfVectorizer(strip_accents='unicode', # remove accents                              \n                             token_pattern=r'\\w{2,}',\n                             ngram_range=(1, 3),\n                             min_df=3,\n                             use_idf=1, \n                             smooth_idf=1, \n                             sublinear_tf=1)\n\nmodels = [\n    ('Naive Bayes', MultinomialNB()),\n    ('Logistic Regression', LogisticRegression(class_weight='balanced', random_state=0, n_jobs=-1)),\n    ('SVM', LinearSVC(class_weight='balanced', random_state=0)),\n    ('KNN', KNeighborsClassifier(n_jobs=-1)),\n    ('Decision Tree', DecisionTreeClassifier(random_state=0, class_weight='balanced')),    \n    ('Random Forest', RandomForestClassifier(n_jobs=-1, random_state=0, class_weight='balanced')), \n    ('Extra Trees', ExtraTreesClassifier(n_jobs=-1, random_state=0, class_weight='balanced')), \n    ('XGBoost', XGBClassifier(random_state=0, n_jobs=-1)), \n    ('LightGBM', LGBMClassifier(objective='multiclass',class_weight='balanced', random_state=0, n_jobs=-1))\n]\n\n# Perform cross-validation\nfor name, model in tqdm(models, leave=None):\n\n    pipeline = Pipeline([('vectorizer', vectorizer),\n                         (name, model)])\n    \n    scores = cross_val_score(pipeline, X, y, scoring='accuracy')\n    \n    acc = np.mean(scores)\n    std = np.std(scores)\n    \n    print(f'Accuracy {acc:.2%} +/- {std:.2%} = {(acc - std):.2%} -- {name}.')","metadata":{"trusted":true},"execution_count":15,"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/9 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Accuracy 59.74% +/- 0.20% = 59.54% -- Naive Bayes.\nAccuracy 62.61% +/- 0.36% = 62.24% -- Logistic Regression.\nAccuracy 62.05% +/- 0.25% = 61.80% -- SVM.\nAccuracy 28.65% +/- 0.95% = 27.70% -- KNN.\nAccuracy 52.70% +/- 0.34% = 52.36% -- Decision Tree.\nAccuracy 61.87% +/- 0.22% = 61.65% -- Random Forest.\nAccuracy 61.45% +/- 0.24% = 61.21% -- Extra Trees.\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/xgboost/sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n","output_type":"stream"},{"name":"stdout","text":"[21:53:59] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/xgboost/sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n","output_type":"stream"},{"name":"stdout","text":"[21:59:32] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/xgboost/sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n","output_type":"stream"},{"name":"stdout","text":"[22:05:04] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/xgboost/sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n","output_type":"stream"},{"name":"stdout","text":"[22:10:36] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/xgboost/sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n","output_type":"stream"},{"name":"stdout","text":"[22:16:09] WARNING: ../src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\nAccuracy 63.49% +/- 0.37% = 63.12% -- XGBoost.\nAccuracy 61.19% +/- 0.20% = 60.99% -- LightGBM.\nCPU times: user 3h 5min 41s, sys: 3min 51s, total: 3h 9min 32s\nWall time: 2h 44min 10s\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}